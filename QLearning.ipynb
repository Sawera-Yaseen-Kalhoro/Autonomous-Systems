{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Robot Learning – Reinforcement Learning**"
      ],
      "metadata": {
        "id": "oTBbTsk4vGkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this practical exercise is to implement a Reinforcement Learning algorithm to learn a policy that moves a robot to a goal position. The algorithm is the Q-learning algorithm.\n",
        "\n",
        "## The Problem\n",
        "The problem consists in finding the goal in a finite 2D environment that is closed and contains some obstacles.\n",
        "\n",
        "**States and actions:** The size of the environment is 20x14=280 states. The robot can only do 4 different actions: {←, ↑, →, ↓} (not diagonal movements!). Therefore the size of the Q function will be 280x4=1120 cells.\n",
        "\n",
        "**Dynamics:** The robot can be located in any free cell (not in the obstacle cells!). The function that describes the dynamics is very simple: the robot will move ONE cell per iteration to the direction of the action that we select, unless there is an obstacle or the wall in front of it, in which case it will\n",
        "stay in the same position.\n",
        "\n",
        "**Reinforcement function:** Since the goal is to reach the goal position as fast as possible, the reinforcement function will give -1 in all cells except in the goal cell, where the reward will be +1. The cell that contains the goal is (3,17)."
      ],
      "metadata": {
        "id": "qCnQbiFVvG0A"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIrgWmuPulz-"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next you have the map that will be used as the environment for the Q-learning algorithm:"
      ],
      "metadata": {
        "id": "9so0pzxsvUyn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CggQ83Bzul0E"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Proposed Map\n",
        "map=[\n",
        "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "[1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1],\n",
        "[1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1],\n",
        "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1],\n",
        "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1],\n",
        "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1],\n",
        "[1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1],\n",
        "[1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "[1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "[1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
        "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1],\n",
        "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1],\n",
        "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
        "\n",
        "\n",
        "# Convert Map 2D array to Numpy array\n",
        "grid_map = np.array(map)\n",
        "\n",
        "# Show grid map\n",
        "plt.matshow(grid_map, cmap = \"jet\")\n",
        "plt.title('Original Map')\n",
        "plt.colorbar()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm\n",
        "\n",
        "In order to implement the Q-learning algorithm, you should follow the next pseudocode:\n",
        "\n",
        "*Initialize Q(s,a) to “0”*\n",
        "\n",
        ">*For n episodes*\n",
        "\n",
        ">>*Initialize s randomly in any free cell*\n",
        "\n",
        ">>*For m iterations repeat*\n",
        "\n",
        ">>>*Choose a following ε-greedy policy*\n",
        "\n",
        ">>>*Take action a, observe r, s’*\n",
        "\n",
        ">>>*Q(s,a)←Q(s,a) + α ( r + γ · maxQ_a’(s’,a’)-Q(s,a))*\n",
        "\n",
        ">>>*s←s’*\n",
        "\n",
        ">>>*if the goal is achieved then finish the episode*\n",
        "\n",
        ">>*endFor*\n",
        "\n",
        "*endFor*\n",
        "\n",
        "\n",
        "You will have to set several parameters experimentally: n, m, ε, α and γ.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "luRN-DT83C7j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_BIOfiSul0H"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the environment class, you set the possible actions, the rewards obtained depending of those actions and the state of the agent after the action is applied.\n",
        "\n",
        "You have to fill the empty functions following the previous pseudocode."
      ],
      "metadata": {
        "id": "45Wso3k5x0ab"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXZAnv09ul0I"
      },
      "outputs": [],
      "source": [
        "class MapEnv:\n",
        "    def __init__(self, map, goal):\n",
        "        self.map = map\n",
        "        self.current_state = self.get_start()\n",
        "        self.goal = goal.astype(np.int32)\n",
        "        self.actions = 4\n",
        "        if map[goal[0], goal[1]] != 0:\n",
        "            raise ValueError(\"Goal position is an obstacle\")\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_state = self.get_start()\n",
        "        return self.current_state\n",
        "\n",
        "    def get_start(self):\n",
        "      # start the agent in a random position within the map:\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "      # this function applies the action taken and returns the next state, the reward and a variable that says if the goal is reached\n",
        "      # action: 0 = up, 1 = down, 2 = left, 3 = right\n",
        "\n",
        "\n",
        "    def render(self, i=0):\n",
        "        plt.matshow(self.map, cmap = \"jet\")\n",
        "        plt.title('Map')\n",
        "        plt.colorbar()\n",
        "        plt.scatter(self.current_state[1], self.current_state[0], c = 'r')\n",
        "        plt.scatter(self.goal[1], self.goal[0], c = 'g')\n",
        "        plt.savefig(\"q_learning_{0:04}.png\".format(i), dpi = 300)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXg1_4u6ul0K"
      },
      "source": [
        "## QLearning algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QLearning class creates and trains the policy at every episode using the information provided by the environment. After the training is over, the optimal policy and the value function are obtained.\n",
        "\n",
        "As in the MapEnv class, you have to fill the empty functions following the previous pseudocode."
      ],
      "metadata": {
        "id": "k7uF89DbFVVq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qnWaAKPul0L"
      },
      "outputs": [],
      "source": [
        "class QLearning:\n",
        "    def __init__(self, env, alpha, gamma, epsilon, n_episodes, n_iterations):\n",
        "        self.env = env\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.n_episodes = n_episodes\n",
        "        self.n_iterations = n_iterations\n",
        "        self.Q = np.random.rand(env.map.shape[0], env.map.shape[1], env.actions)\n",
        "\n",
        "    def epsilon_greedy_policy(self, s, epsilon):\n",
        "      # Epsilon greedy policy (choose a random action with probability epsilon)\n",
        "\n",
        "\n",
        "    def episode(self, alpha, epsilon):\n",
        "      # Episode execution for n_iterations. Generate an action with epsilon_greedy_policy,\n",
        "\n",
        "\n",
        "    def train(self, check_every_n_episodes=200, average_n_episodes=50):\n",
        "        \"\"\"Execute n_episodes and every 200 episodes stop training in order to retrieve\n",
        "            the average reward for 100 episodes, then resume training\"\"\"\n",
        "\n",
        "        accumulated_rewards = []\n",
        "        for i in range(self.n_episodes):\n",
        "            self.episode(self.alpha, self.epsilon)\n",
        "\n",
        "            if i % check_every_n_episodes == 0:\n",
        "                accum_reward = 0\n",
        "                for j in range(average_n_episodes):\n",
        "                    total_reward = self.episode(0, 0)\n",
        "                    accum_reward += total_reward\n",
        "                accumulated_rewards.append(accum_reward / average_n_episodes)\n",
        "                print(accumulated_rewards[-1], end=\" \")\n",
        "\n",
        "        return accumulated_rewards\n",
        "\n",
        "    def get_optimal_policy(self):\n",
        "        \"\"\"Retrieve the optimal policy from Q(s,a)\"\"\"\n",
        "        policy = np.argmax(self.Q, axis = 2)\n",
        "        policy[self.env.map == 1] = -1\n",
        "        policy[self.env.goal[0], self.env.goal[1]] = -2\n",
        "        return policy\n",
        "\n",
        "    def value_function(self):\n",
        "        \"\"\"Retrieve the optimal value function from from Q(s,a)\"\"\"\n",
        "        v = np.max(self.Q, axis = 2)\n",
        "        v[self.env.map == 1] = -np.inf\n",
        "        return v\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHXBD26aul0N"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the training you need to choose some parameters experimentally. Uncomment the parameters and test different values to see how the training results change.\n",
        "\n",
        "Parameters:\n",
        "\n",
        "*alpha*: learning rate of the algorithm\n",
        "\n",
        "*gamma*: discount factor of the algorithm\n",
        "\n",
        "*epsilon*: random action probability\n",
        "\n",
        "*n_episodes*: number of episode repetitions\n",
        "\n",
        "*n_iterations*: number of iterations per episode"
      ],
      "metadata": {
        "id": "7U2V6wf_-ZVP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLtF2eDUul0O"
      },
      "outputs": [],
      "source": [
        "env = MapEnv(grid_map, np.array([3, 18]))\n",
        "#alpha =\n",
        "#gamma =\n",
        "#epsilon =\n",
        "#n_episodes =\n",
        "#n_iterations =\n",
        "ql = QLearning(env, alpha, gamma, epsilon, n_episodes, n_iterations)\n",
        "rewards = ql.train()\n",
        "plt.plot(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJWZblvUul0O"
      },
      "source": [
        "## Plot value function and optimal policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_z8566Oul0P"
      },
      "outputs": [],
      "source": [
        "v = ql.value_function()\n",
        "plt.matshow(v, cmap = \"jet\")\n",
        "plt.title('Value Function')\n",
        "plt.colorbar()\n",
        "\n",
        "policy = ql.get_optimal_policy()\n",
        "plt.matshow(policy, cmap = \"jet\")\n",
        "plt.title('Optimal Policy: 0 = up, 1 = down, 2 = left, 3 = right')\n",
        "plt.colorbar()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0HhgeIAul0Q"
      },
      "source": [
        "## Test current Policy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the training is over, we can see what the robot has learnt to do. You can test it with other goal positions and other maps, to see of the policy is able to adapt to other situations."
      ],
      "metadata": {
        "id": "To2nTQUn_HGM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTD9Hsxiul0Q"
      },
      "outputs": [],
      "source": [
        "env = MapEnv(grid_map, np.array([3, 17]))\n",
        "i = 0\n",
        "\n",
        "for j in range(1):\n",
        "    done = False\n",
        "    s = env.reset()\n",
        "    while not done:\n",
        "        a = policy[s[0], s[1]]\n",
        "        s, reward, done, _ = env.step(a)\n",
        "        env.render(i)\n",
        "        i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "\n",
        "You must deliver this Python Interactive Notebook. The file must have the name q_learning_YOUR_NAME.ipynb. Also, you must do a report commenting the problems you encountered, a discussion on how the parameters affect the training and conclusions for the results obtained.\n",
        "\n",
        "Make sure that all cells can be executed."
      ],
      "metadata": {
        "id": "KKfb29-0BkCV"
      }
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "e8efbb1c39583597a5dcee4af3da3e7c09711a134bb0c2976f954bb2a2f0e8df"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('venv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}